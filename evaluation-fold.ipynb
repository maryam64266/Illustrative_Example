{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "255a046e05404536",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clusters, tags, sentiment\n",
    "tags_c = pd.read_csv('../dataset/final_clusters.csv')\n",
    "tags = pd.read_csv(\"../dataset/tags_withglovevec.csv\")\n",
    "df_s = pd.read_csv('../dataset/df_tag_sentiment.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7950fef131ced877",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# By Fold Evaluation Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd92ba665e67a49f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dce0ff635c3b81ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculation of similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    '''Cosine similarity function only computes similarity for NON-NAN values'''\n",
    "    # Indices where both v1 and v2 are not NaN\n",
    "    shared_idx = np.where(~np.isnan(v1) & ~np.isnan(v2))\n",
    "\n",
    "    # If no shared indices, return 0\n",
    "    if len(shared_idx[0]) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Extract shared values\n",
    "    v1_shared = v1[shared_idx]\n",
    "    v2_shared = v2[shared_idx]\n",
    "\n",
    "    # Compute the dot product and norms only on shared values\n",
    "    dot_product = np.dot(v1_shared, v2_shared)\n",
    "    norm_v1 = np.linalg.norm(v1_shared)\n",
    "    norm_v2 = np.linalg.norm(v2_shared)\n",
    "\n",
    "    # Prevent division by zero\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return 0\n",
    "\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "\n",
    "def get_sentiment_label(sentiment):\n",
    "    if sentiment > 0.5:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.5:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "def get_dominant_sentiment(user_sentiments):\n",
    "    pos_count = sum(1 for s in user_sentiments if s > 0.5)\n",
    "    neg_count = sum(1 for s in user_sentiments if s < -0.5)\n",
    "    neutral_count = len(user_sentiments) - pos_count - neg_count\n",
    "\n",
    "    if pos_count > neg_count and pos_count > neutral_count:\n",
    "        return \"positive\"\n",
    "    elif neg_count > pos_count and neg_count > neutral_count:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "def get_k_nearest_neighbors(target_user, matrix, k):\n",
    "    similarities = {}\n",
    "\n",
    "    for user in matrix.index:\n",
    "        if user == target_user:\n",
    "            continue\n",
    "        common_movies = matrix.loc[target_user].dropna().index.intersection(matrix.loc[user].dropna().index)\n",
    "        if len(common_movies) > 0:\n",
    "            user_vector = matrix.loc[target_user][common_movies].values.reshape(1, -1)\n",
    "            target_vector = matrix.loc[user][common_movies].values.reshape(1, -1)\n",
    "            sim = cosine_similarity(user_vector, target_vector)\n",
    "\n",
    "            similarities[user] = sim\n",
    "\n",
    "    # Sort by similarity\n",
    "    sorted_neighbors = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_neighbors[:k]\n",
    "\n",
    "\n",
    "def get_distributed_recommendations(target_user, matrix, new_to_original_index, df_sent, k):\n",
    "    # Get the sentiments for movies rated by the target user\n",
    "    user_sentiments = matrix.loc[target_user].dropna()\n",
    "\n",
    "    # Determine the dominant sentiment of the target user\n",
    "    dominant_sentiment = get_dominant_sentiment(user_sentiments)\n",
    "\n",
    "    # Get the k-most similar users\n",
    "    similar_users = get_k_nearest_neighbors(target_user, matrix, k)\n",
    "    sentiment_buckets = defaultdict(list)\n",
    "    # For each similar user, gather movies they've rated\n",
    "\n",
    "    for user, similarity in similar_users:\n",
    "        # Map the user index to its original userId\n",
    "        original_user_id = new_to_original_index[user]\n",
    "\n",
    "        for movie, sentiment in matrix.loc[user].items():\n",
    "            if not np.isnan(sentiment):\n",
    "\n",
    "                # Filter df_sent to get rows for the current user and movie\n",
    "                user_movie_df = df_sent[(df_sent['userId'] == original_user_id) & (df_sent['movieId'] == movie)]\n",
    "\n",
    "                # If there are no sentiments for this user-movie pair, continue\n",
    "                if user_movie_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Aggregate the scaled_sentiment_values. Here, I'm using mean, but you can adjust this\n",
    "                movie_sentiment = user_movie_df['scaled_sentiment_value'].mean()\n",
    "\n",
    "                sentiment_direction = get_dominant_sentiment([movie_sentiment])\n",
    "                sentiment_buckets[sentiment_direction].append((movie, movie_sentiment))\n",
    "\n",
    "    return sentiment_buckets[dominant_sentiment][:k]  # returns movieId and sentiment value - scaled sentiment value from df_sent\n",
    "\n",
    "\n",
    "def generate_and_store_recommendations(df_rec, mat, new_to_original_index, df_sent, k=10):\n",
    "    for idx, row in df_rec.iterrows():\n",
    "        original_user_id = row['userId']\n",
    "\n",
    "        # Map the original userId to the new continuous index\n",
    "        target_user = original_to_new_index.get(original_user_id, None)\n",
    "\n",
    "        # If the user exists in the matrix\n",
    "        if target_user is not None:\n",
    "            # Get recommendations\n",
    "            recommendations = get_distributed_recommendations(target_user, mat, new_to_original_index, df_sent, k)\n",
    "\n",
    "            # Store recommendations in df_rec under the original userId\n",
    "            df_rec.at[idx, 'recommendations'] = recommendations\n",
    "\n",
    "\n",
    "# Define sentiment category function\n",
    "def determine_sentiment_category(sentiment_value):\n",
    "    if -1 <= sentiment_value < -0.5:\n",
    "        return 'Negative'\n",
    "    elif -0.5 <= sentiment_value <= 0.5:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "\n",
    "# Extract tags function\n",
    "def get_tags_for_movie(movieId, sentiment_value):\n",
    "    sentiment_category = determine_sentiment_category(sentiment_value)\n",
    "\n",
    "    # Filter rows with matching movieId and sentiment category\n",
    "    tags = df_comb_t[(df_comb_t['movieId'] == movieId) &\n",
    "                     (df_comb_t['scaled_sentiment_value_avg'].apply(\n",
    "                         determine_sentiment_category) == sentiment_category)]['tag'].tolist()\n",
    "\n",
    "    # Convert tags list to set and back to list to ensure distinct tags\n",
    "    distinct_tags = list(set(tags))\n",
    "\n",
    "    return (movieId, distinct_tags)\n",
    "\n",
    "def get_user_data(df_rec, tag_cv, target_userId):\n",
    "    \"\"\"\n",
    "    Retrieve movie recommendations, user tags, and tags_movies for a target userId.\n",
    "\n",
    "    Parameters:\n",
    "        df_rec (DataFrame): The DataFrame containing userId, recommendations, and tags_movies columns.\n",
    "        tag_cv (DataFrame): The DataFrame containing userId, movieId, and tags.\n",
    "        target_userId (int): The userId for whom the data is to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists:\n",
    "               - The first list contains recommendations (movieId, sentiment) for the target userId.\n",
    "               - The second list contains tags applied by the target userId in the format (movieId, tag).\n",
    "               - The third list contains tags_movies entries for the target userId in the format [(movieId, [tags...]), ...].\n",
    "    \"\"\"\n",
    "    # Fetch movie recommendations\n",
    "    recommendations = get_user_recommendations(df_rec, target_userId)\n",
    "\n",
    "    # Fetch tags applied by the user\n",
    "    user_tags = get_user_tags(tag_cv, target_userId)\n",
    "\n",
    "    # Fetch tags_movies data for the user\n",
    "    tags_movies = get_tags_movies_for_user(df_rec, target_userId)\n",
    "\n",
    "    return recommendations, user_tags, tags_movies\n",
    "\n",
    "\n",
    "def get_user_recommendations(df_rec, target_userId):\n",
    "    filtered_df = df_rec[df_rec['userId'] == target_userId]\n",
    "    if len(filtered_df) == 0:\n",
    "        return []\n",
    "    recommendations = filtered_df['recommendations'].iloc[0]\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def get_user_tags(tag_cv, target_userId):\n",
    "    filtered_df = tag_cv[tag_cv['userId'] == target_userId]\n",
    "    if len(filtered_df) == 0:\n",
    "        return []\n",
    "    user_tags = filtered_df[['movieId', 'tag']].values.tolist()\n",
    "    return user_tags\n",
    "\n",
    "\n",
    "def get_tags_movies_for_user(df_rec, target_userId):\n",
    "    filtered_df = df_rec[df_rec['userId'] == target_userId]\n",
    "    if len(filtered_df) == 0:\n",
    "        return []\n",
    "    tags_movies = filtered_df['tags_movies'].iloc[0]\n",
    "    return tags_movies\n",
    "\n",
    "\n",
    "def get_user_feature_set_and_wsd(df_feat, df_wsd, user_tags, tag_clusters, target_userId):\n",
    "    \"\"\"\n",
    "    Fetch additional features, WSD, and TF/TF-IDF info for a list of user tags.\n",
    "    \n",
    "    Parameters:\n",
    "        df_feat (DataFrame): The DataFrame containing features for each tag.\n",
    "        df_wsd (DataFrame): The DataFrame containing WSD information.\n",
    "        user_tags (list): The list containing tags applied by the user in the format (movieId, tag).\n",
    "        target_userId (int): The userId for whom the WSD data is to be fetched.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames:\n",
    "               - The first DataFrame contains additional features for each user tag.\n",
    "               - The second DataFrame contains WSD information for the user.\n",
    "               - The third DataFrame contains TF and TF-IDF information for the user tags.\n",
    "    \"\"\"\n",
    "    # Filtering df_feat to only include rows where the tag is in user_tags\n",
    "    filtered_df_feat = df_feat[df_feat['tag'].isin([tag for _, tag in user_tags])]\n",
    "\n",
    "    # Filtering df_wsd to include only rows for the target_userId\n",
    "    filtered_df_wsd = df_wsd[df_wsd['userId'] == target_userId]\n",
    "\n",
    "    # joining the cluster to the tags\n",
    "\n",
    "    # Create a set of (movieId, tag) tuples for easier filtering\n",
    "    user_tags_set = set(tuple(x) for x in user_tags)\n",
    "\n",
    "    return filtered_df_feat, filtered_df_wsd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity_for_recommendations(recommendations, tags_movies, df_feat, user_features, df_wsd, user_wsd):\n",
    "    similarity_list = []\n",
    "\n",
    "    # Create a dictionary to easily fetch movie tags\n",
    "    tags_movies_dict = dict(tags_movies)\n",
    "\n",
    "    for movieId, _ in recommendations:\n",
    "        movie_tags = tags_movies_dict.get(movieId, [])  # Fetch the tags from the tags_movies dictionary\n",
    "        if not movie_tags:\n",
    "            continue\n",
    "\n",
    "        for user_tag in user_features['tag'].unique():\n",
    "            for movie_tag in movie_tags:\n",
    "\n",
    "                user_wsd_row = user_wsd[user_wsd['tag'] == user_tag]\n",
    "                movie_wsd_row = df_wsd[(df_wsd['movieId'] == movieId) & (df_wsd['tag'] == movie_tag)]\n",
    "\n",
    "                # nested loop to calculate the wsd\n",
    "                wsd_similarities = []\n",
    "                for user_sense in user_wsd_row['disambiguated_sense'].values:\n",
    "                    for movie_sense in movie_wsd_row['disambiguated_sense'].values:\n",
    "                        wsd_similarities.append(user_sense == movie_sense)\n",
    "\n",
    "                if wsd_similarities:\n",
    "                    wsd_similarity = np.mean(wsd_similarities)\n",
    "                else:\n",
    "                    wsd_similarity = 0\n",
    "\n",
    "                wsd_confidence = movie_wsd_row['confidence'].mean() if not movie_wsd_row.empty else 0\n",
    "\n",
    "                user_feat_row = user_features[user_features['tag'] == user_tag]\n",
    "                movie_feat_row = df_feat[df_feat['tag'] == movie_tag]\n",
    "\n",
    "                pos_match = (user_feat_row['POS'].values == movie_feat_row['POS'].values).mean() if user_feat_row['POS'].values.size and movie_feat_row['POS'].values.size else 0\n",
    "                ner_match = (user_feat_row['ner_label'].values == movie_feat_row['ner_label'].values).mean() if user_feat_row['ner_label'].values.size and movie_feat_row['ner_label'].values.size else 0\n",
    "                sentiment_match = (user_feat_row['sentiment_label'].values == movie_feat_row['sentiment_label'].values).mean() if user_feat_row['sentiment_label'].values.size and movie_feat_row['sentiment_label'].values.size else 0\n",
    "\n",
    "                # Check for cluster match\n",
    "                cluster_match = int(user_feat_row['cluster'].values[0] == movie_feat_row['cluster'].values[0]) if user_feat_row['cluster'].values.size and movie_feat_row['cluster'].values.size else 0\n",
    "\n",
    "                similarity_list.append({\n",
    "                    'movieId': movieId,\n",
    "                    'user_tag': user_tag,\n",
    "                    'movie_tag': movie_tag,\n",
    "                    'wsd_similarity': wsd_similarity,\n",
    "                    'wsd_confidence': wsd_confidence,\n",
    "                    'pos_match': pos_match,\n",
    "                    'ner_match': ner_match,\n",
    "                    'sentiment_match': sentiment_match,\n",
    "                    'cluster_match': cluster_match  # Adding the cluster match to the dataframe\n",
    "                })\n",
    "\n",
    "    similarity_df = pd.DataFrame(similarity_list)\n",
    "    return similarity_df\n",
    "\n",
    "\n",
    "def calculate_total_similarity(row, weights):\n",
    "    return sum(row[metric] * weight for metric, weight in weights.items())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f69bafe13d86a07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# reading in fold 1-5\n",
    "for fold in range(1,5):\n",
    "    df_fold_train_1 = pd.read_csv(f\"../dataset/evaluation_folds/fold_{fold}_train.csv\")\n",
    "    tags_c = pd.read_csv('../dataset/final_clusters.csv')\n",
    "    tags = pd.read_csv(\"../dataset/tags_withglovevec.csv\")\n",
    "    df_s = pd.read_csv('../dataset/df_tag_sentiment.csv')\n",
    "    \n",
    "    # data conversions\n",
    "    tags_c['tag'] = tags_c['tag'].astype('str')\n",
    "    df_s['tag'] = df_s['tag'].astype('str')\n",
    "        \n",
    "    # combine df_comb and tags_c\n",
    "    df_comb = tags_c.merge(df_s, on='tag', how='left')\n",
    "    \n",
    "    # drop duplicates on (tag, movie) -> so the cluster average is not bias to duplicate tags per movie\n",
    "    df_comb.drop_duplicates(subset='tag', inplace=True)\n",
    "    # df_comb\n",
    "    avg_sentiment_by_cluster = df_comb.groupby('cluster')['scaled_sentiment_value'].mean().reset_index()\n",
    "    \n",
    "    df_comb = pd.merge(df_comb, avg_sentiment_by_cluster, on='cluster', suffixes=('', '_cluster_avg'))\n",
    "    df_comb = df_comb.drop(columns=['glove_vec'])\n",
    "    df_comb['scaled_sentiment_value_avg'] = df_comb['scaled_sentiment_value_cluster_avg']\n",
    "    df_mat = tags.merge(df_comb, on=['tag'], how='inner')\n",
    "    \n",
    "    df_sent = df_mat  # take copy for later on \n",
    "  \n",
    "    #### CF generate recommendations and store in a file here - \n",
    "    # df_fold_train_1\n",
    "    # tags_c\n",
    "    tags_c['cluster'] = tags_c['new_cluster']\n",
    "    tags_c.drop(columns='new_cluster', inplace=True)\n",
    "    \n",
    "    df_s = pd.read_csv('../dataset/df_tag_sentiment.csv')\n",
    "    \n",
    "    # data conversions\n",
    "    tags_c['tag'] = tags_c['tag'].astype('str')\n",
    "    df_s['tag'] = df_s['tag'].astype('str')\n",
    "        \n",
    "    # combine df_comb and tags_c\n",
    "    df_comb = tags_c.merge(df_s, on='tag', how='left')\n",
    "    \n",
    "    # drop duplicates on (tag, movie) -> so the cluster average is not bias to duplicate tags per movie\n",
    "    df_comb.drop_duplicates(subset='tag', inplace=True)\n",
    "    \n",
    "    avg_sentiment_by_cluster = df_comb.groupby('cluster')['scaled_sentiment_value'].mean().reset_index()\n",
    "    \n",
    "    df_comb = pd.merge(df_comb, avg_sentiment_by_cluster, on='cluster', suffixes=('', '_cluster_avg'))\n",
    "    \n",
    "    df_mat = df_fold_train_1.merge(df_comb, on=['tag'], how='inner')\n",
    "    df_sent = df_mat\n",
    "    # df_mat\n",
    "    df_mat['scaled_sentiment_value_avg'] = df_mat['scaled_sentiment_value_cluster_avg']\n",
    "    \n",
    "    # Convert the 'scaled_sentiment_value_avg' column to float\n",
    "    df_mat['scaled_sentiment_value_avg'] = df_mat['scaled_sentiment_value_avg'].astype('float')\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    df_mat = df_mat[[\"userId\", \"movieId\", \"scaled_sentiment_value_avg\"]]\n",
    "    \n",
    "    # Check for duplicates and print them\n",
    "    duplicates = df_mat[df_mat.duplicated(subset=['userId', 'movieId'], keep=False)]\n",
    "    \n",
    "    # Group by 'userId' and 'movieId' to get the average 'scaled_sentiment_value_avg'\n",
    "    df_mat = df_mat.groupby(['userId', 'movieId'])['scaled_sentiment_value_avg'].mean().reset_index()\n",
    "    \n",
    "    # Create the pivot table\n",
    "    mat = pd.pivot_table(df_mat, values='scaled_sentiment_value_avg', index=['userId'], columns=['movieId'])\n",
    "    \n",
    "    # Assuming 'mat' is your matrix\n",
    "    # Resetting the index will add a column 'userId' with the original values\n",
    "    mat = mat.reset_index()\n",
    "    \n",
    "    # creating a dictionary with original userId and new continuous index\n",
    "    original_to_new_index = {old_id: new_id for new_id, old_id in enumerate(mat['userId'])}\n",
    "    \n",
    "    # assign new continous index to the userId column \n",
    "    mat['userId'] = mat['userId'].map(original_to_new_index)\n",
    "    \n",
    "    # Now, set 'userId' as the index again\n",
    "    mat.set_index('userId', inplace=True)\n",
    "    \n",
    "    user_indices = mat.index[mat.apply(lambda x: (x.count() == 1) and (x.dropna().iloc[0] > 0.5), axis=1)]\n",
    "    \n",
    "    new_to_original_index = {v: k for k, v in\n",
    "                             original_to_new_index.items()}  # this exists outside the function to ensure the new_to_original_index exists for mapping\n",
    "    \n",
    "    original_to_new_index = {v: k for k, v in new_to_original_index.items()}\n",
    "    df_comb['scaled_sentiment_value_avg'] = df_comb['scaled_sentiment_value_cluster_avg']\n",
    "    # creation of df_rec dataframe \n",
    "    \n",
    "    df_rec = df_sent[[\"userId\"]].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Initialize recommendations column in df_rec\n",
    "    df_rec['recommendations'] = None\n",
    "    \n",
    "    # Generate and store recommendations\n",
    "    df_rec['recommendations'] = generate_and_store_recommendations(df_rec, mat, new_to_original_index, df_sent)\n",
    "    df_rec['recommendations'] = df_rec['recommendations'].apply(lambda x: [(int(a), round(b, 4)) for a, b in x])\n",
    "    \n",
    "    df_comb_t = pd.merge(tags, df_comb, on='tag', how='left')\n",
    "    df_comb_t = df_comb_t[[\"movieId\", \"tag\", \"scaled_sentiment_value\", \"scaled_sentiment_value_avg\"]]\n",
    "    \n",
    "    # getting initial tags recommendation \n",
    "    # Apply function on recommendations column\n",
    "    df_rec['tags_movies'] = df_rec['recommendations'].apply(\n",
    "        lambda recs: [get_tags_for_movie(movieId, sentiment_value) for movieId, sentiment_value in recs])\n",
    "    df_rec.to_json(f\"../dataset/evaluation/cf_train_{fold}_recs.json\")\n",
    "    \n",
    "    \n",
    "    df_feat = pd.read_json(\"../dataset/df_feat.json\")\n",
    "    tag_cv = pd.read_json('../dataset/tag_csv.json')\n",
    "    tag_clusters = pd.read_csv(\"../dataset/tag_clusters.csv\")\n",
    "    \n",
    "    df_wsd = pd.read_csv(\"../dataset/df_wsd.csv\")\n",
    "    # df_wsd  # can directly filter on this\n",
    "    # df_feat  # need to merge on tag_clusters\n",
    "    df_feat_cl = pd.merge(df_feat, tag_clusters, on=['tag'], how='left')\n",
    "\n",
    "            \n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    # df_feat\n",
    "    \n",
    "    test_data = pd.read_csv(f'../dataset/evaluation_folds/fold_{fold}_test.csv')\n",
    "    train_data = df_fold_train_1\n",
    "    \n",
    "    df_fold_1_results = pd.DataFrame(columns=['userId', 'recommended_tags'])\n",
    "    \n",
    "    ############ CB MODEL ############################\n",
    "    \n",
    "    merged_tag_cv = pd.merge(tag_cv, test_data, on=['userId', 'movieId', 'tag'], how='outer', indicator=True)\n",
    "    \n",
    "    # Now, filter out the rows that come only from the 'test_data' using the indicator column.\n",
    "    tag_cv_filtered = merged_tag_cv[merged_tag_cv['_merge'] != 'right_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    # Repeat the process for df_wsd.\n",
    "    merged_df_wsd = pd.merge(df_wsd, test_data, on=['userId', 'movieId', 'tag'], how='outer', indicator=True)\n",
    "    df_wsd_filtered = merged_df_wsd[merged_df_wsd['_merge'] != 'right_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    for userId in train_data['userId'].unique():\n",
    "        try:\n",
    "            target_user = userId\n",
    "    \n",
    "            recommendations, user_tags, tags_movies = get_user_data(df_rec, tag_cv_filtered, target_user)\n",
    "    \n",
    "            user_features, user_wsd = get_user_feature_set_and_wsd(df_feat, df_wsd_filtered, user_tags, tag_clusters,\n",
    "                                                                   target_user)\n",
    "    \n",
    "            similarity_df = calculate_similarity_for_recommendations(recommendations, tags_movies, df_feat, user_features,\n",
    "                                                                     df_wsd, user_wsd)\n",
    "    \n",
    "            weights = {\n",
    "                'wsd_similarity': 0.05,\n",
    "                'wsd_confidence': 0.05,\n",
    "                'pos_match': 0.3,\n",
    "                'ner_match': 0.3,\n",
    "                'sentiment_match': 0.2,\n",
    "                'cluster_match': 0.1\n",
    "            }\n",
    "    \n",
    "            # Add a check for empty DataFrame\n",
    "            if similarity_df.empty:\n",
    "                print(f\"No recommendations for user {userId}, adding empty recommendation.\")\n",
    "                recommended_tags = []\n",
    "            else:\n",
    "                # Assuming similarity_df is your DataFrame\n",
    "                similarity_df['total_similarity'] = similarity_df.apply(\n",
    "                    lambda row: calculate_total_similarity(row, weights), axis=1)\n",
    "    \n",
    "                # Sort the DataFrame by 'total_similarity' in descending order\n",
    "                sorted_similarity_df = similarity_df.sort_values(by='total_similarity', ascending=False)\n",
    "    \n",
    "                # Remove rows where 'movie_tag' is also present in 'user_tag' column\n",
    "                filtered_similarity_df = sorted_similarity_df[~sorted_similarity_df['movie_tag'].isin(user_tags)]\n",
    "    \n",
    "                # Drop duplicates based on the 'movie_tag' column to get distinct tags\n",
    "                distinct_tags_df = filtered_similarity_df.drop_duplicates(subset=['movie_tag'])\n",
    "    \n",
    "                # Get the top 10 distinct tags along with their movieId\n",
    "                top_10_distinct_tags_with_movieId = distinct_tags_df[\n",
    "                    ['movieId', 'movie_tag', 'ner_match', 'pos_match', 'cluster_match', 'total_similarity']].head(10)\n",
    "                # Calculate the average similarity of the top 10 tags\n",
    "                average_similarity = top_10_distinct_tags_with_movieId[\n",
    "                    'total_similarity'].mean() if not top_10_distinct_tags_with_movieId.empty else 0\n",
    "    \n",
    "                recommended_tags = top_10_distinct_tags_with_movieId['movie_tag'].tolist()\n",
    "    \n",
    "            # Now, we can add the results to df_fold_1_results DataFrame, regardless of whether recommendations are empty or not.\n",
    "            temp_df = pd.DataFrame({\n",
    "                'userId': [userId],  # Make sure to pass a list even if it's a single userId\n",
    "                'recommended_tags': [recommended_tags],\n",
    "                # The brackets ensure it's treated as a single list entry in the DataFrame\n",
    "                'average_similarity': [average_similarity]  # Add the average similarity for the top 10 tags\n",
    "            })\n",
    "    \n",
    "            # Concatenate the temporary DataFrame with the main results DataFrame\n",
    "            df_fold_1_results = pd.concat([df_fold_1_results, temp_df], ignore_index=True)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for user {userId}: {e}\")\n",
    "    \n",
    "    ### RESULTs\n",
    "    df_fold_1_results.to_json(f'../dataset/evaluation/df_fold_{fold}_results.json')\n",
    "    print(f'Finished Recommendation fold {fold}')\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ff84ecfc3476200"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate Precision@K, Recall@K, and NDCG@K\n",
    "def calculate_metrics_at_k(test_data_path, results_data_path, k):\n",
    "    # Load the data\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    df_results = pd.read_json(results_data_path)\n",
    "\n",
    "    # Process the test data into a mapping of userId to a list of actual tags\n",
    "    test_tags_dict = defaultdict(set)\n",
    "    for _, row in test_data.iterrows():\n",
    "        test_tags_dict[row['userId']].add(row['tag'])\n",
    "\n",
    "    # Initialize lists to store individual metrics\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    ndcg_list = []\n",
    "\n",
    "    for _, row in df_results.iterrows():\n",
    "        user_id = row['userId']\n",
    "        # Take the top K recommended tags\n",
    "        recommended_tags = set(row['recommended_tags'][:k])  # Top K tags\n",
    "        actual_tags = test_tags_dict.get(user_id, set())\n",
    "        k_len = min(len(actual_tags),k)\n",
    "\n",
    "\n",
    "        # # Precision@K\n",
    "        individual_precisions = []\n",
    "        for tag in recommended_tags:\n",
    "            if tag in actual_tags:\n",
    "                individual_precisions.append(1)  # Relevant tag in the top K recommendations\n",
    "            else:\n",
    "                individual_precisions.append(0)  # Non-relevant tag in the top K recommendations\n",
    "\n",
    "        precision_at_k = sum(individual_precisions) / k_len  # Average precision\n",
    "        precision_list.append(precision_at_k)\n",
    "\n",
    "        # Recall@K\n",
    "        relevant_tags = set(recommended_tags).intersection(actual_tags)\n",
    "        if len(actual_tags) > 0:\n",
    "            recall_at_k = len(relevant_tags) / len(actual_tags)\n",
    "        else:\n",
    "            recall_at_k = 1\n",
    "        recall_list.append(recall_at_k)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_precision_at_k = sum(precision_list) / len(precision_list) if precision_list else 0\n",
    "    overall_recall_at_k = sum(recall_list) / len(recall_list) if recall_list else 0\n",
    "\n",
    "    return overall_precision_at_k, overall_recall_at_k\n",
    "\n",
    "\n",
    "# Function to calculate Coverage@K\n",
    "def calculate_coverage_at_k(test_data_path, results_data_path, k):\n",
    "    all_possible_tags = set()\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    all_possible_tags.update(test_df['tag'].unique())\n",
    "\n",
    "    df_results = pd.read_json(results_data_path)\n",
    "    recommended_tags_set = set()\n",
    "\n",
    "    for _, row in df_results.iterrows():\n",
    "        recommended_tags = row['recommended_tags'][:k]\n",
    "        recommended_tags_set.update(recommended_tags)\n",
    "\n",
    "    coverage_at_k = len(recommended_tags_set) / len(all_possible_tags) if all_possible_tags else 0\n",
    "    # print(\"all unique tags\",len(all_possible_tags))\n",
    "    return coverage_at_k\n",
    "\n",
    "\n",
    "# Function to calculate Long-Tail Coverage@K\n",
    "def calculate_LTcoverage_at_k(test_data_path, results_data_path, k):\n",
    "    df_results = pd.read_json(results_data_path)\n",
    "    df_test = pd.read_csv(test_data_path)\n",
    "\n",
    "    # Get all tags from the test data and their frequencies\n",
    "    test_tags = df_test['tag']\n",
    "    tag_counts_test = Counter(test_tags)\n",
    "    df_freq_test = pd.DataFrame(tag_counts_test.items(), columns=['Tag', 'Frequency'])\n",
    "\n",
    "    if df_freq_test.empty:\n",
    "        return 0.0  # Avoid division by zero if no test tags\n",
    "\n",
    "    # Identify long-tail tags based on the frequency quantile in the test data\n",
    "    threshold = df_freq_test['Frequency'].quantile(0.8)\n",
    "    long_tail_tags = set(df_freq_test[df_freq_test['Frequency'] <= threshold]['Tag'])\n",
    "\n",
    "    if not long_tail_tags:\n",
    "        return 1.0 # If no long-tail tags, consider coverage as 100%\n",
    "\n",
    "    # Collect all recommended tags up to k for all users\n",
    "    recommended_tags_at_k = set()\n",
    "    for _, row in df_results.iterrows():\n",
    "        recommended_tags = row['recommended_tags'][:k]\n",
    "        recommended_tags_at_k.update(recommended_tags)\n",
    "\n",
    "    # Calculate the intersection of recommended tags and long-tail tags\n",
    "    covered_long_tail_tags = recommended_tags_at_k.intersection(long_tail_tags)\n",
    "\n",
    "    # Calculate long-tail coverage\n",
    "    long_tail_coverage = len(covered_long_tail_tags) / len(long_tail_tags) if long_tail_tags else 0.0\n",
    "    return long_tail_coverage\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b502e5b5da3e9293"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metrics_data = []\n",
    "# Set K\n",
    "k_values = range(1,11)\n",
    "\n",
    "# Loop through fold_0 to fold_4\n",
    "for fold_num in range(5):\n",
    "    test_data_path = f'../dataset/evaluation_folds/fold_{fold_num}_test.csv'\n",
    "    results_data_path = f'../dataset/evaluation/testNMF_train_{fold_num}_recs.json'\n",
    "\n",
    "    pre_k = []\n",
    "    rec_k = []\n",
    "    cov_k = []\n",
    "    ltcov_k = []\n",
    "\n",
    "    # Loop through each value of k and calculate precision and recall\n",
    "    for k in k_values:\n",
    "        # Calculate precision, recall, and NDCG for this fold and value of K\n",
    "        pre, rec  = calculate_metrics_at_k(test_data_path, results_data_path, k)\n",
    "        cov = calculate_coverage_at_k(test_data_path,results_data_path, k)\n",
    "        ltcov = calculate_LTcoverage_at_k(test_data_path,results_data_path, k)\n",
    "\n",
    "        # Append the results for this value of k\n",
    "        pre_k.append(pre)\n",
    "        rec_k.append(rec)\n",
    "        cov_k.append(cov)\n",
    "        ltcov_k.append(ltcov)\n",
    "        # ndcg_k.append(ndcg)\n",
    "\n",
    "    # Append results to the metrics data list\n",
    "    metrics_data.append({\n",
    "        'Fold': fold_num,\n",
    "        **{f'P@{k}': pre_k[i] for i, k in enumerate(k_values)},  # Add Precision values for each k\n",
    "        **{f'R@{k}': rec_k[i] for i, k in enumerate(k_values)},  # Add Recall values for each k\n",
    "        **{f'F1@{k}': (2*rec_k[i]*pre_k[i])/(rec_k[i]+pre_k[i]) for i, k in enumerate(k_values)},  # Add Recall values for each k\n",
    "        **{f'Cov@{k}': cov_k[i] for i, k in enumerate(k_values)},  # Add Recall values for each k\n",
    "        **{f'LTCov@{k}': ltcov_k[i] for i, k in enumerate(k_values)},  # Add Recall values for each k\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the collected metrics data\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Add a row for the overall average metrics across all folds\n",
    "avg_row = {\n",
    "    'Fold': 'Avg.',\n",
    "    **{f'P@{k}': df_metrics[f'P@{k}'].mean() for i, k in enumerate(k_values)},\n",
    "    **{f'R@{k}': df_metrics[f'R@{k}'].mean() for i, k in enumerate(k_values)},\n",
    "    **{f'F1@{k}': df_metrics[f'F1@{k}'].mean() for i, k in enumerate(k_values)},\n",
    "    **{f'Cov@{k}': df_metrics[f'Cov@{k}'].mean() for i, k in enumerate(k_values)},\n",
    "    **{f'LTCov@{k}': df_metrics[f'LTCov@{k}'].mean() for i, k in enumerate(k_values)},\n",
    "}\n",
    "\n",
    "std_row = {\n",
    "    'Fold': 'Std.',\n",
    "    **{f'P@{k}': df_metrics[f'P@{k}'].std() for i, k in enumerate(k_values)},\n",
    "    **{f'R@{k}': df_metrics[f'R@{k}'].std() for i, k in enumerate(k_values)},\n",
    "    **{f'F1@{k}': df_metrics[f'F1@{k}'].std() for i, k in enumerate(k_values)},\n",
    "    **{f'Cov@{k}': df_metrics[f'Cov@{k}'].std() for i, k in enumerate(k_values)},\n",
    "    **{f'LTCov@{k}': df_metrics[f'LTCov@{k}'].std() for i, k in enumerate(k_values)},\n",
    "}\n",
    "\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([std_row])], ignore_index=True)\n",
    "\n",
    "# Output the table\n",
    "df_metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f1cacda800e088",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
